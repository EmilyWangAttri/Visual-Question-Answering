prefix: san_sgd
verbose: True
mode: 'train'
save_dir: '/home/cse/phd/csz178058/Visual-Question-Answering/outputs'
use_gpu: True
model_class: san
debug: False
seed: 123213
data:
    path: '/home/cse/phd/csz178058/scratch/vqadata/'
    random_seed: 1
    #shuffle: False
    questions_path: 'v2_OpenEnded_mscoco_val2014_questions.json'
    annotation_path: 'v2_mscoco_val2014_annotations.json'
    custom_batch_size: 64
    features_dir: 'train2014_san_i_1024_448'
    scale_params: [448,448]
    crop_params: 448
    loader_params: 
        batch_size: 1
        num_workers: 4
        pin_memory: True

checkpoints:
    path: 'checkpoints.pth.tar'
    best_model_path: 'best_model_checkpoint.pth.tar'

stats:
    trainTimeLogs: 'trainingTime.csv'
    accuracyLogs: 'accuracy.csv'
    outputOverValidation: 'validationTags.txt'


optim:
    class: rmsprop
    scheduler: 'CustomReduceLROnPlateau'
    scheduler_params:
        maxPatienceToStopTraining: 16
        base_class_params:
            mode: 'min'
            factor: 0.1
            patience: 7
            verbose: True 
            threshold: 0.02
            threshold_mode: 'rel' 
            cooldown: 0 
            min_lr: 0.00001
            eps: 0.000001
    params: 
        momentum: 0.9
        lr: 0.0004      # learning rate
        alpha: 0.99     # alpha for adagrad/rmsprop/momentum/adam
        beta: 0.995     # beta used for adam
        eps: 1e-8       # epsilon that goes into denominator in rmsprop
        weight_decay: 0.0005
    clip: 0.1

training:
    no_of_epochs: 100
    start_from_checkpoint: True
    write_output_to_file: True

model:
    params:
        word_emb_size: 300
        emb_size: 1024
        att_ff_size: 512
        num_att_layers: 1
        num_mlp_layers: 1
